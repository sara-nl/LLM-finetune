#!/bin/bash

#SBATCH --job-name=finetune_llama
#SBATCH --partition=gpu_h100
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=1
#SBATCH --cpus-per-task=16
#SBATCH --time=12:00:00


source venv/bin/activate
export HF_HOME=/scratch-shared/$USER/.cache_dir/
python finetune_unsloth.py \
                          --pretrained_model_name_or_path unsloth/Meta-Llama-3.1-8B-bnb-4bit \
                          --data_dir Open-Orca/SlimOrca \
                          --output_dir /scratch-shared/$USER/finetune_results/ \
                          --max_seq_length 8192 \
                          --per_device_train 8 \
                          --per_device_eval 8 \
                          --num_train_epochs 1 \
                          --optim adamw_8bit \
                          --bf16 \
                          --gradient_accumulation_steps 4 \
                          --packing \
                          --logging_steps 100 \
